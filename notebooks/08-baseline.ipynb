{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import attr\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import islice\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, path, skim=None):\n",
    "        self.path = path\n",
    "        self.skim = skim\n",
    "        \n",
    "    def lines(self):\n",
    "        with open(self.path) as fh:\n",
    "            for line in fh:\n",
    "                yield line.strip()\n",
    "    \n",
    "    def abstract_lines(self):\n",
    "        lines = []\n",
    "        for line in self.lines():\n",
    "            if line:\n",
    "                lines.append(line)\n",
    "            else:\n",
    "                yield lines\n",
    "                lines = []\n",
    "\n",
    "    def abstracts(self):\n",
    "        ab_lines = self.abstract_lines()\n",
    "        if self.skim:\n",
    "            ab_lines = islice(ab_lines, self.skim)\n",
    "        for lines in ab_lines:\n",
    "            yield Abstract.from_lines(lines)\n",
    "            \n",
    "    def xy(self, vocab):\n",
    "        for abstract in self.abstracts():\n",
    "            yield from abstract.xy(vocab)\n",
    "            \n",
    "    def token_counts(self):\n",
    "        counts = defaultdict(lambda: 0)\n",
    "        for ab in self.abstracts():\n",
    "            for tokens in ab.sentence_tokens():\n",
    "                for token in tokens:\n",
    "                    counts[token] += 1\n",
    "        return Counter(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class Abstract:\n",
    "    \n",
    "    identifier = attr.ib()\n",
    "    tags = attr.ib()\n",
    "    sentences = attr.ib()\n",
    "    \n",
    "    @classmethod\n",
    "    def from_lines(cls, lines):\n",
    "        return cls(lines[0], lines[1].split(), lines[2:])\n",
    "    \n",
    "    def sentence_tokens(self):\n",
    "        for sent in self.sentences:\n",
    "            yield re.findall('[a-z]+', sent.lower())\n",
    "    \n",
    "    def xy(self, vocab):\n",
    "        for y, tokens in enumerate(self.sentence_tokens()):\n",
    "            x = Counter([t for t in tokens if t in vocab])\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Corpus('../data/abstracts/train.txt', 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = train.token_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([k for k, _ in counts.most_common(2000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = zip(*train.xy(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = dv.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4758878x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 83076373 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Corpus('../data/abstracts/test.txt', 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = zip(*test.xy(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = dv.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21033759596707113"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(test_y, fit.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = dv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidx = fit.coef_.argsort()\n",
    "eidx = np.flip(fit.coef_.argsort(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.04391251609 let\n",
      "-0.839570262001 report\n",
      "-0.821624699506 often\n",
      "-0.744173888617 investigate\n",
      "-0.736926258363 aim\n",
      "-0.72042082595 article\n",
      "-0.710025531718 widely\n",
      "-0.702355226738 recently\n",
      "-0.700656541372 investigated\n",
      "-0.696710484944 paper\n",
      "-0.688601878405 usually\n",
      "-0.649508898501 consider\n",
      "-0.603694138812 telescope\n",
      "-0.591801665283 studied\n",
      "-0.585823048807 recent\n",
      "-0.573573314178 study\n",
      "-0.548325052036 goal\n",
      "-0.547469191432 qcd\n",
      "-0.540713387226 theoretically\n",
      "-0.5247422734 attention\n",
      "-0.523751509071 collider\n",
      "-0.523110668401 review\n",
      "-0.518296486362 superconductor\n",
      "-0.515212021721 electroweak\n",
      "-0.509930582242 phys\n",
      "-0.508077087246 superconductors\n",
      "-0.499669826799 note\n",
      "-0.495807320365 supersymmetric\n",
      "-0.474015518876 presents\n",
      "-0.472864090316 globular\n",
      "-0.459139291805 crystals\n",
      "-0.454792581225 known\n",
      "-0.453939958781 purpose\n",
      "-0.452874395955 survey\n",
      "-0.446413121966 examine\n",
      "-0.438447377713 universe\n",
      "-0.433179944735 parton\n",
      "-0.422473808713 relativity\n",
      "-0.41966849567 supersymmetry\n",
      "-0.416090744843 branching\n",
      "-0.415194041589 present\n",
      "-0.405929789729 laser\n",
      "-0.401279048296 superconducting\n",
      "-0.40003733375 nearby\n",
      "-0.399318953017 supergravity\n",
      "-0.396369220524 inequality\n",
      "-0.388862869823 microwave\n",
      "-0.387398772818 mesons\n",
      "-0.38184708214 inequalities\n",
      "-0.380575988542 newton\n"
     ]
    }
   ],
   "source": [
    "for i in bidx[:50]:\n",
    "    print(fit.coef_[i], names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4959325565 finally\n",
      "1.88096397355 conclude\n",
      "1.24508366065 furthermore\n",
      "1.09707015432 suggests\n",
      "1.02540010775 also\n",
      "1.00784665863 moreover\n",
      "0.965459172661 further\n",
      "0.905914423004 suggest\n",
      "0.889823427602 findings\n",
      "0.883119498333 addition\n",
      "0.868217642158 implications\n",
      "0.851651339355 therefore\n",
      "0.843390390501 illustrate\n",
      "0.839950329613 indicates\n",
      "0.800677596931 our\n",
      "0.766996185709 thus\n",
      "0.765746433514 likely\n",
      "0.68659055861 suggesting\n",
      "0.652716057733 could\n",
      "0.647259823769 confirm\n",
      "0.638348489099 future\n",
      "0.59035618229 would\n",
      "0.580813368898 overall\n",
      "0.575410019273 might\n",
      "0.556805393588 consistent\n",
      "0.545282955054 art\n",
      "0.543424534093 tested\n",
      "0.541094731761 appears\n",
      "0.538489506626 indeed\n",
      "0.537078707391 companion\n",
      "0.534550125838 will\n",
      "0.533981919464 better\n",
      "0.524746294705 hence\n",
      "0.524567758745 confirmed\n",
      "0.522371775071 then\n",
      "0.522296846586 another\n",
      "0.520586677892 instead\n",
      "0.516657969146 estimated\n",
      "0.511777121631 agreement\n",
      "0.503175337279 msun\n",
      "0.496264220058 example\n",
      "0.490508057525 similar\n",
      "0.486380125119 contrast\n",
      "0.482750040874 find\n",
      "0.475776829625 demonstrate\n",
      "0.474517946584 briefly\n",
      "0.470698133858 simulated\n",
      "0.46971278318 indicating\n",
      "0.467041104315 explained\n",
      "0.465938067475 examples\n"
     ]
    }
   ],
   "source": [
    "for i in eidx[:50]:\n",
    "    print(fit.coef_[i], names[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
